# backend/app/workers/overlay_worker.py
"""
Overlay Worker for background overlay generation processing.

Handles:
- Background processing of overlay generation jobs
- Priority-based job queue processing
- Retry logic with exponential backoff
- Job status updates and error handling
- Integration with existing OverlayService for actual generation
"""

import asyncio
import time
from typing import Optional, List, TypedDict
from datetime import datetime, timedelta

from .base_worker import BaseWorker, WorkerErrorResponse
from ..utils.time_utils import utc_now
from ..services.overlay_pipeline.services.job_service import SyncOverlayJobService
from ..services.overlay_pipeline import OverlayService
from ..database.sse_events_operations import SyncSSEEventsOperations
from ..database.core import SyncDatabase
from ..services.settings_service import SyncSettingsService
from ..services.weather.service import WeatherManager
from ..models.overlay_model import OverlayGenerationJob
from ..enums import OverlayJobStatus, OverlayJobPriority, SSEPriority
from ..constants import (
    DEFAULT_OVERLAY_JOB_BATCH_SIZE,
    DEFAULT_OVERLAY_WORKER_INTERVAL,
    DEFAULT_OVERLAY_MAX_RETRIES,
    DEFAULT_OVERLAY_CLEANUP_HOURS,
    OVERLAY_JOB_RETRY_DELAYS,
)


class OverlayWorker(BaseWorker):
    """
    High-performance background worker for overlay generation job processing.

    The OverlayWorker implements a sophisticated job processing pipeline that
    decouples overlay generation from the critical RTSP capture workflow,
    ensuring that overlay processing never impacts capture timing or reliability.

    Core Responsibilities:
    - Priority-based job queue processing (high > medium > low)
    - Concurrent overlay generation with configurable limits
    - Adaptive performance scaling based on queue load
    - Comprehensive retry logic with exponential backoff
    - Real-time progress broadcasting via SSE events
    - Automatic cleanup of completed jobs
    - Performance monitoring and statistics collection

    Integration Architecture:
    - Integrates with existing OverlayService infrastructure
    - Coordinates with CaptureWorker via job queuing
    - Updates image overlay status in real-time
    - Broadcasts job lifecycle events for frontend updates

    Error Handling:
    - Maximum 3 retry attempts with exponential backoff (1min, 5min, 15min)
    - Graceful degradation on overlay generation failures
    - Comprehensive error logging for debugging
    - Automatic job cleanup after completion/failure

    Thread Safety:
    - Designed for synchronous worker environments
    - Safe concurrent execution with proper locking
    - No shared mutable state between worker instances
    """

    def __init__(
        self,
        sync_db: SyncDatabase,
        settings_service: SyncSettingsService,
        weather_manager: Optional[WeatherManager] = None,
        worker_interval: int = DEFAULT_OVERLAY_WORKER_INTERVAL,
        batch_size: int = DEFAULT_OVERLAY_JOB_BATCH_SIZE,
        max_retries: int = DEFAULT_OVERLAY_MAX_RETRIES,
        cleanup_hours: int = DEFAULT_OVERLAY_CLEANUP_HOURS,
    ):
        """
        Initialize overlay worker with dependencies and configuration.

        Args:
            sync_db: Synchronous database connection
            settings_service: Settings service for configuration access
            weather_manager: Weather manager for weather data access
            worker_interval: Seconds between job processing cycles
            batch_size: Number of jobs to process per cycle
            max_retries: Maximum retry attempts for failed jobs
            cleanup_hours: Hours after which completed jobs are cleaned up
        """
        super().__init__("OverlayWorker")

        # Core services
        self.overlay_job_service = SyncOverlayJobService(db=sync_db, settings_service=settings_service)
        self.overlay_service = OverlayService(
            db=sync_db,
            settings_service=settings_service,
            weather_manager=weather_manager,
        )
        self.sse_ops = SyncSSEEventsOperations(sync_db)

        # Configuration
        self.interval = worker_interval
        self.batch_size = batch_size
        self.max_retries = max_retries
        self.cleanup_hours = cleanup_hours
        self.weather_manager = weather_manager

        # Add job_service reference for compatibility
        self.job_service = self.overlay_job_service

        # Performance tracking
        self.processed_jobs = 0
        self.failed_jobs = 0
        self.last_cleanup = utc_now()
        self.processing_times = []

        self.log_info(
            f"🎨 OverlayWorker initialized with batch_size={batch_size}, "
            f"interval={worker_interval}s, max_retries={max_retries}"
        )

    async def initialize(self) -> None:
        """
        Initialize worker-specific resources.

        Sets up overlay generation services and prepares the worker
        for processing overlay jobs.
        """
        try:
            self.log_info("Initializing overlay worker resources...")

            # Initialize start time for statistics
            self.start_time = utc_now()

            # Validate that overlay service is properly configured
            if not self.overlay_service:
                raise ValueError("OverlayService not properly initialized")

            # Validate that job service is properly configured
            if not self.job_service:
                raise ValueError("OverlayJobService not properly initialized")

            self.log_info("✅ Overlay worker initialization complete")

        except Exception as e:
            self.log_error("Failed to initialize overlay worker", e)
            raise

    async def cleanup(self) -> None:
        """
        Cleanup worker-specific resources.

        Performs graceful shutdown of overlay processing, ensuring
        any in-progress jobs are handled appropriately.
        """
        try:
            self.log_info("Starting overlay worker cleanup...")

            # Log final statistics
            status = self.get_status()
            self.log_info(
                f"Final worker stats: {status['processed_jobs']} processed, "
                f"{status['failed_jobs']} failed, "
                f"{status['success_rate']:.1f}% success rate"
            )

            # Clear processing times to free memory
            self.processing_times.clear()

            self.log_info("✅ Overlay worker cleanup complete")

        except Exception as e:
            self.log_error("Error during overlay worker cleanup", e)
            # Don't re-raise during cleanup to allow graceful shutdown

    async def process_jobs(self) -> None:
        """
        Main job processing cycle.

        Retrieves pending jobs, processes them with proper error handling,
        and updates job status. Includes performance monitoring and cleanup.
        """
        try:
            # Get pending jobs
            pending_jobs = self.overlay_job_service.get_pending_jobs(self.batch_size)

            if not pending_jobs:
                self.log_debug("No pending overlay jobs to process")
                await self._perform_maintenance()
                return

            self.log_info(f"🎨 Processing {len(pending_jobs)} overlay jobs")

            # Process jobs with performance tracking
            start_time = time.time()
            processed_count = 0

            for job in pending_jobs:
                try:
                    success = await self._process_single_job(job)
                    if success:
                        processed_count += 1
                        self.processed_jobs += 1
                    else:
                        self.failed_jobs += 1

                except Exception as e:
                    self.log_error(
                        f"Unexpected error processing overlay job {job.id}: {e}"
                    )
                    self._mark_job_failed(job.id, f"Unexpected error: {str(e)}")
                    self.failed_jobs += 1

            # Performance tracking
            cycle_time = time.time() - start_time
            self.processing_times.append(cycle_time)

            # Keep only last 10 measurements for rolling average
            if len(self.processing_times) > 10:
                self.processing_times = self.processing_times[-10:]

            avg_time = sum(self.processing_times) / len(self.processing_times)

            self.log_info(
                f"🎨 Completed overlay job cycle: {processed_count}/{len(pending_jobs)} successful, "
                f"cycle_time={cycle_time:.2f}s, avg_time={avg_time:.2f}s"
            )

            # Broadcast processing statistics via SSE
            await self._broadcast_statistics()

        except Exception as e:
            self.log_error(f"Error in overlay job processing cycle: {e}")

    async def _process_single_job(self, job: OverlayGenerationJob) -> bool:
        """
        Process a single overlay generation job.

        Args:
            job: Overlay generation job to process

        Returns:
            True if job was successfully processed
        """
        try:
            job_start_time = time.time()

            # Mark job as processing
            if not self.overlay_job_service.mark_job_processing(job.id):
                self.log_warning(f"Failed to mark overlay job {job.id} as processing")
                return False

            self.log_debug(f"Processing overlay job {job.id} for image {job.image_id}")

            # Generate overlay using the overlay service
            success = self.overlay_service.generate_overlay_for_image(
                image_id=job.image_id,
                force_regenerate=True,  # Always regenerate in worker context
            )

            processing_time = time.time() - job_start_time

            if success:
                # Mark job as completed
                if self.overlay_job_service.mark_job_completed(job.id):
                    self.log_info(
                        f"✅ Completed overlay job {job.id} for image {job.image_id} "
                        f"in {processing_time:.2f}s"
                    )

                    # Broadcast completion event
                    await self._broadcast_job_completed(job, processing_time)
                    return True
                else:
                    self.log_error(f"Failed to mark overlay job {job.id} as completed")
                    return False
            else:
                # Job failed - handle retry logic
                return await self._handle_job_failure(job, "Overlay generation failed")

        except Exception as e:
            self.log_error(f"Error processing overlay job {job.id}: {e}")
            return await self._handle_job_failure(job, str(e))

    async def _handle_job_failure(
        self, job: OverlayGenerationJob, error_message: str
    ) -> bool:
        """
        Handle job failure with retry logic.

        Args:
            job: Failed job
            error_message: Error description

        Returns:
            True if job was queued for retry, False if permanently failed
        """
        try:
            # Check if job can be retried
            if job.retry_count < self.max_retries:
                # Schedule retry
                if self.overlay_job_service.schedule_retry(job.id):
                    retry_delay = OVERLAY_JOB_RETRY_DELAYS[
                        min(job.retry_count, len(OVERLAY_JOB_RETRY_DELAYS) - 1)
                    ]

                    self.log_warning(
                        f"⚠️ Overlay job {job.id} failed (attempt {job.retry_count + 1}), "
                        f"scheduled for retry in {retry_delay} minutes: {error_message}"
                    )

                    # Broadcast retry event
                    await self._broadcast_job_retry(job, error_message, retry_delay)
                    return True
                else:
                    self.log_error(f"Failed to schedule retry for overlay job {job.id}")

            # Maximum retries exceeded or retry scheduling failed
            self._mark_job_failed(job.id, error_message)
            self.log_error(
                f"❌ Overlay job {job.id} permanently failed after {job.retry_count} retries: {error_message}"
            )

            # Broadcast permanent failure event
            await self._broadcast_job_failed(job, error_message)
            return False

        except Exception as e:
            self.log_error(f"Error handling overlay job failure: {e}")
            self._mark_job_failed(job.id, f"Error handling failure: {str(e)}")
            return False

    def _mark_job_failed(self, job_id: int, error_message: str) -> bool:
        """Mark a job as permanently failed."""
        try:
            return self.overlay_job_service.mark_job_failed(job_id, error_message)
        except Exception as e:
            self.log_error(f"Error marking overlay job {job_id} as failed: {e}")
            return False

    async def _perform_maintenance(self) -> None:
        """
        Perform periodic maintenance tasks.

        Includes job cleanup, retry scheduling, and performance monitoring.
        """
        try:
            # Cleanup old completed jobs (every hour)
            if utc_now() - self.last_cleanup > timedelta(hours=1):
                await self._cleanup_completed_jobs()
                self.last_cleanup = utc_now()

            # Process jobs eligible for retry
            await self._process_retry_jobs()

        except Exception as e:
            self.log_error(f"Error in overlay worker maintenance: {e}")

    async def _cleanup_completed_jobs(self) -> None:
        """Clean up completed and failed jobs older than cleanup threshold."""
        try:
            cleaned_count = self.overlay_job_service.cleanup_completed_jobs(
                self.cleanup_hours
            )
            if cleaned_count > 0:
                self.log_info(f"🧹 Cleaned up {cleaned_count} completed overlay jobs")

        except Exception as e:
            self.log_error(f"Error cleaning up overlay jobs: {e}")

    async def _process_retry_jobs(self) -> None:
        """Process jobs that are eligible for retry."""
        try:
            retry_jobs = self.overlay_job_service.get_jobs_for_retry()

            if retry_jobs:
                self.log_info(
                    f"🔄 Found {len(retry_jobs)} overlay jobs eligible for retry"
                )

                for job in retry_jobs:
                    # Reset job to pending status for retry
                    if self.overlay_job_service.schedule_retry(job.id):
                        self.log_debug(f"Scheduled overlay job {job.id} for retry")

        except Exception as e:
            self.log_error(f"Error processing retry overlay jobs: {e}")

    async def _broadcast_statistics(self) -> None:
        """Broadcast job processing statistics via SSE."""
        try:
            stats = self.overlay_job_service.get_job_statistics()

            # Create SSE event for statistics
            event_data = {
                "total_jobs_24h": stats.total_jobs_24h,
                "pending_jobs": stats.pending_jobs,
                "processing_jobs": stats.processing_jobs,
                "completed_jobs_24h": stats.completed_jobs_24h,
                "failed_jobs_24h": stats.failed_jobs_24h,
                "worker_processed": self.processed_jobs,
                "worker_failed": self.failed_jobs,
                "avg_processing_time_ms": stats.avg_processing_time_ms,
                "timestamp": utc_now().isoformat(),
            }

            self.sse_ops.create_event(
                event_type="overlay_worker_statistics",
                event_data=event_data,
                priority=SSEPriority.LOW,
                source="overlay_worker",
            )

        except Exception as e:
            self.log_warning(f"Failed to broadcast overlay worker statistics: {e}")

    async def _broadcast_job_completed(
        self, job: OverlayGenerationJob, processing_time: float
    ) -> None:
        """Broadcast job completion event via SSE."""
        try:
            event_data = {
                "job_id": job.id,
                "image_id": job.image_id,
                "processing_time_ms": int(processing_time * 1000),
                "priority": job.priority,
                "timestamp": utc_now().isoformat(),
            }

            self.sse_ops.create_event(
                event_type="overlay_generation_completed",
                event_data=event_data,
                priority=SSEPriority.NORMAL,
                source="overlay_worker",
            )

        except Exception as e:
            self.log_warning(f"Failed to broadcast overlay job completion: {e}")

    async def _broadcast_job_retry(
        self, job: OverlayGenerationJob, error_message: str, retry_delay: int
    ) -> None:
        """Broadcast job retry event via SSE."""
        try:
            event_data = {
                "job_id": job.id,
                "image_id": job.image_id,
                "retry_count": job.retry_count + 1,
                "error_message": error_message,
                "retry_delay_minutes": retry_delay,
                "timestamp": utc_now().isoformat(),
            }

            self.sse_ops.create_event(
                event_type="overlay_generation_retry",
                event_data=event_data,
                priority=SSEPriority.NORMAL,
                source="overlay_worker",
            )

        except Exception as e:
            self.log_warning(f"Failed to broadcast overlay job retry: {e}")

    async def _broadcast_job_failed(
        self, job: OverlayGenerationJob, error_message: str
    ) -> None:
        """Broadcast job permanent failure event via SSE."""
        try:
            event_data = {
                "job_id": job.id,
                "image_id": job.image_id,
                "retry_count": job.retry_count,
                "error_message": error_message,
                "timestamp": utc_now().isoformat(),
            }

            self.sse_ops.create_event(
                event_type="overlay_generation_failed",
                event_data=event_data,
                priority=SSEPriority.NORMAL,
                source="overlay_worker",
            )

        except Exception as e:
            self.log_warning(f"Failed to broadcast overlay job failure: {e}")

    async def run(self) -> None:
        """
        Main worker loop - processes overlay jobs continuously while running.

        This method should be called after start() to begin job processing.
        According to the CEO architecture, this worker is part of the Media Department
        and processes overlay generation jobs on demand.
        """
        self.log_info("Starting overlay worker main loop")

        while self.running:
            try:
                # Process pending overlay jobs
                await self.process_jobs()

                # Sleep before next cycle
                await asyncio.sleep(self.interval)

            except asyncio.CancelledError:
                # Handle graceful shutdown
                self.log_info("Overlay worker loop cancelled")
                break
            except Exception as e:
                self.log_error(f"Unexpected error in overlay worker loop: {e}")
                # Continue after error with a short delay
                await asyncio.sleep(5)

        self.log_info("Overlay worker main loop stopped")

    def get_status(self) -> dict:
        """
        Get current worker status and statistics.

        Returns:
            Dictionary with worker status information
        """
        avg_processing_time = (
            sum(self.processing_times) / len(self.processing_times)
            if self.processing_times
            else 0
        )

        return {
            "name": self.name,
            "running": self.running,
            "processed_jobs": self.processed_jobs,
            "failed_jobs": self.failed_jobs,
            "success_rate": (
                (self.processed_jobs / (self.processed_jobs + self.failed_jobs)) * 100
                if (self.processed_jobs + self.failed_jobs) > 0
                else 0
            ),
            "avg_processing_time": avg_processing_time,
            "batch_size": self.batch_size,
            "interval": self.interval,
            "last_cleanup": self.last_cleanup.isoformat(),
            "uptime": (
                (utc_now() - self.start_time).total_seconds()
                if hasattr(self, "start_time")
                else 0
            ),
        }
